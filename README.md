# llamaCPPServerVisual
llama.cpp server running InternVL-2.5-1b locally
